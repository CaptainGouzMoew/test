{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40962836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "import global_settings\n",
    "importlib.reload(global_settings)\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0802bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory found: D:\\Software\\temp\\CeDX\\data_2024_05/data\n",
      "Directory found: D:\\Software\\temp\\CeDX\\data_2024_05/data/tmp\n",
      "Directory found: D:\\Software\\temp\\CeDX\\data_2024_05/data/tmp2\n",
      "Directory found: D:\\Software\\temp\\CeDX\\data_2024_05/output\n"
     ]
    }
   ],
   "source": [
    "# Define working folder\n",
    "\n",
    "setts = global_settings.Settings()\n",
    "\n",
    "root_folder = setts.root_folder\n",
    "tmp1_folder = setts.tmp1_folder\n",
    "tmp2_folder = setts.tmp2_folder\n",
    "output_folder = setts.output_folder\n",
    "\n",
    "# tmp1_folder = root_folder + \"/data/tmp\"\n",
    "# tmp2_folder = root_folder + \"/data/tmp2\"\n",
    "# input_folder = root_folder + \"/input\"\n",
    "# output_folder = root_folder + \"/output\"\n",
    "\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "    \n",
    "year_limit = setts.year_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db759e75",
   "metadata": {},
   "source": [
    "# Add Customer Info from sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d426c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "import unicodedata\n",
    "\n",
    "def replace_tp(tp):\n",
    "    tp = tp.replace('THANH HOÁ', 'THANH HÓA')\n",
    "    tp = tp.replace('THÀNH PHỐTHANH HÓA', 'THANH HÓA')\n",
    "    tp = tp.replace('KHÁNH HOÀ', 'KHÁNH HÒA')\n",
    "    tp = tp.replace('TP.HCM', 'HỒ CHÍ MINH')\n",
    "    tp = tp.replace('TPHCM', 'HỒ CHÍ MINH')\n",
    "    tp = tp.replace('TP HCM', 'HỒ CHÍ MINH')\n",
    "    tp = tp.replace('HCM', 'HỒ CHÍ MINH')\n",
    "    tp = tp.replace('HỒ CHÍ MINH', 'HỒ CHÍ MINH')\n",
    "    tp = tp.replace('DAKLAK', 'ĐẮK LẮK')\n",
    "    tp = tp.replace('ÐẮK LẮK', 'ĐẮK LẮK')\n",
    "    tp = tp.replace('ÐỒNG NAI', 'ĐỒNG NAI')\n",
    "    tp = tp.replace('ÐỒNG NAI', 'ĐỒNG NAI')\n",
    "    tp = tp.replace('ÐÀ NẴNG', 'ĐÀ NẴNG')\n",
    "    tp = tp.replace('HOÀ BÌNH', 'HÒA BÌNH')\n",
    "    tp = tp.replace('THỪA THIÊN HUẾ', 'THỪA THIÊN - HUẾ')\n",
    "    tp = tp.replace('BÌNH ÐỊNH', 'BÌNH ĐỊNH')\n",
    "    tp = tp.replace('ÐỒNG THÁP', 'ĐỒNG THÁP')\n",
    "    tp = tp.replace('HN', 'HÀ NỘI')\n",
    "    tp = tp.replace('HÀ NỘI', 'HÀ NỘI')\n",
    "    tp = tp.replace('HUẾ', 'THỪA THIÊN - HUẾ')\n",
    "    tp = tp.replace('APP', '')\n",
    "    tp = tp.replace('(', '')\n",
    "    tp = tp.replace(')', '')\n",
    "    #tp = tp.replace('Ð', 'Đ')\n",
    "    #tp = tp.replace('ÒA', 'OÀ')\n",
    "    #tp = tp.replace('OÀ', 'ÒA')\n",
    "    #tp = re.sub(r\"[^a-zA-Z0-9]+\", ' ', tp)\n",
    "    return tp\n",
    "\n",
    "def no_accent_vietnamese(s):\n",
    "    s = re.sub(r'[àáạảãâầấậẩẫăằắặẳẵà]', 'a', s)\n",
    "    s = re.sub(r'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪÀ]', 'A', s)\n",
    "    s = re.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
    "    s = re.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
    "    s = re.sub(r'[òóọỏõôồốộổỗơờớợởỡộ]', 'o', s)\n",
    "    s = re.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠỘ]', 'O', s)\n",
    "    s = re.sub(r'[ìíịỉĩ]', 'i', s)\n",
    "    s = re.sub(r'[ÌÍỊỈĨ]', 'I', s)\n",
    "    s = re.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
    "    s = re.sub(r'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
    "    s = re.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
    "    s = re.sub(r'[ỲÝỴỶỸ]', 'Y', s)\n",
    "    s = re.sub(r'[Đ]', 'D', s)\n",
    "    s = re.sub(r'[đ]', 'd', s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', s)\n",
    "    return s\n",
    "\n",
    "# def no_accent_vietnamese(input_string):\n",
    "#     normalized_string = ''.join(\n",
    "#         unicodedata.normalize('NFD', char)\n",
    "#         for char in input_string\n",
    "#         if unicodedata.category(char) != 'Mn'\n",
    "#     )\n",
    "#     return normalized_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_city(df, city_col_in, city_col_out):\n",
    "    vn_city = (\n",
    "    \"Cà Mau\",\n",
    "    \"Bạc Liêu\",\n",
    "    \"Sóc Trăng\",\n",
    "    \"Hậu Giang\",\n",
    "    \"Cần Thơ\",\n",
    "    \"Kiên Giang\",\n",
    "    \"An Giang\",\n",
    "    \"Đồng Tháp\",\n",
    "    \"Vĩnh Long\",\n",
    "    \"Trà Vinh\",\n",
    "    \"Bến Tre\",\n",
    "    \"Tiền Giang\",\n",
    "    \"Long An\",\n",
    "    \"Hồ Chí Minh\",\n",
    "    \"Bà Rịa\",\n",
    "    \"Vũng Tàu\",\n",
    "    \"Bà Rịa - Vũng Tàu\",\n",
    "    \"Đồng Nai\",\n",
    "    \"Bình Dương\",\n",
    "    \"Tây Ninh\",\n",
    "    \"Bình Phước\",\n",
    "    \"Lâm Đồng\",\n",
    "    \"Đắk Nông\",\n",
    "    \"Đắk Lắk\",\n",
    "    \"Gia Lai\",\n",
    "    \"Kon Tum\",\n",
    "    \"Bình Thuận\",\n",
    "    \"Ninh Thuận\",\n",
    "    \"Khánh Hòa\",\n",
    "    \"Phú Yên\",\n",
    "    \"Bình Định\",\n",
    "    \"Quảng Ngãi\",\n",
    "    \"Quảng Nam\",\n",
    "    \"Đà Nẵng\",\n",
    "    \"Huế\",\n",
    "    \"Quảng Trị\",\n",
    "    \"Quảng Bình\",\n",
    "    \"Hà Tĩnh\",\n",
    "    \"Nghệ An\",\n",
    "    \"Thanh Hóa\",\n",
    "    \"Ninh Bình\",\n",
    "    \"Nam Định\",\n",
    "    \"Hà Nam\",\n",
    "    \"Thái Bình\",\n",
    "    \"Hưng Yên\",\n",
    "    \"Hải Phòng\",\n",
    "    \"Hải Dương\",\n",
    "    \"Hà Nội\",\n",
    "    \"Bắc Ninh\",\n",
    "    \"Vĩnh Phúc\",\n",
    "    \"Phú Thọ\",\n",
    "    \"Bắc Giang\",\n",
    "    \"Quảng Ninh\",\n",
    "    \"Lạng Sơn\",\n",
    "    \"Thái Nguyên\",\n",
    "    \"Hòa Bình\",\n",
    "    \"Yên Bái\",\n",
    "    \"Sơn La\",\n",
    "    \"Lai Châu\",\n",
    "    \"Điện Biên\",\n",
    "    \"Lào Cai\",\n",
    "    \"Tuyên Quang\",\n",
    "    \"Bắc Kạn\",\n",
    "    \"Cao Bằng\",\n",
    "    \"Hà Giang\",\n",
    "    )\n",
    "    city_master_list = vn_city\n",
    "    address_list = tuple(df[city_col_in])\n",
    "    thanhpho_list = []\n",
    "    for add in address_list:\n",
    "        has_city = ''\n",
    "        add = add.upper()\n",
    "        add = re.sub(' +', ' ', add).strip()\n",
    "        add_og = add\n",
    "        for thanh_pho in city_master_list:\n",
    "            add = replace_tp(add)\n",
    "            if add[-len(thanh_pho):] == thanh_pho.upper():\n",
    "                has_city = thanh_pho.upper()\n",
    "                break\n",
    "\n",
    "        if has_city == '':\n",
    "            for thanh_pho in city_master_list:\n",
    "                temp = difflib.SequenceMatcher(None, no_accent_vietnamese(add_og), no_accent_vietnamese(thanh_pho).upper())\n",
    "                if temp.ratio() >= 0.9:\n",
    "                    has_city = thanh_pho.upper()\n",
    "                    break\n",
    "                else:\n",
    "                    if no_accent_vietnamese(thanh_pho).upper() in no_accent_vietnamese(add_og).upper():\n",
    "                        has_city = thanh_pho.upper()\n",
    "                        break\n",
    "\n",
    "        thanhpho_list.append(has_city)\n",
    "    df[city_col_out] = thanhpho_list\n",
    "\n",
    "    df[city_col_out] = df[city_col_out].apply(lambda x: x.replace('Bà Rịa - Vũng Tàu'.upper(), 'BRVT'.upper())\n",
    "                                  .replace('Bà Rịa'.upper(), 'BRVT'.upper())\n",
    "                                  .replace('Vũng Tàu'.upper(), 'BRVT'.upper()))\n",
    "\n",
    "    df[city_col_out] = df[city_col_out].apply(lambda x: x.replace( 'BRVT'.upper(), 'Bà Rịa - Vũng Tàu'.upper()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443c3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_numeric_chars(input_string):\n",
    "    return ''.join(char for char in input_string if char.isdigit())\n",
    "\n",
    "def modify_phone_num_column(df, col):\n",
    "    df[col] = df[col].apply(lambda x: remove_non_numeric_chars(str(x))).dropna()\n",
    "    df = df[df[col] != '']\n",
    "    df[col] = df[col].astype(np.longlong).astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5b12a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AO_HVN_ITS_LABO_DWH\\AppData\\Local\\Temp\\ipykernel_22884\\3644766933.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sale = pd.read_csv(root_folder + '/data/CeDX_Sale.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           MALE\n",
      "1           MALE\n",
      "2           MALE\n",
      "3           MALE\n",
      "4               \n",
      "           ...  \n",
      "234400      MALE\n",
      "234401      MALE\n",
      "234402      MALE\n",
      "234403      MALE\n",
      "234404    FEMALE\n",
      "Name: GENDER, Length: 234405, dtype: object\n",
      "0           LẠNG SƠN\n",
      "1             HÀ NỘI\n",
      "2             HÀ NỘI\n",
      "3             HÀ NỘI\n",
      "4             HÀ NỘI\n",
      "             ...    \n",
      "234400    BÌNH DƯƠNG\n",
      "234401     THANH HÓA\n",
      "234402     VĨNH PHÚC\n",
      "234403     VĨNH LONG\n",
      "234404     HẢI PHÒNG\n",
      "Name: CITY, Length: 234405, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sale = pd.read_csv(root_folder + '/data/CeDX_Sale.csv')\n",
    "sale['GENDER'] = sale['MALE'].astype('str').str.replace('X', 'MALE') + sale['FEMALE'].astype('str').str.replace('X', 'FEMALE')\n",
    "sale['GENDER'] = sale['GENDER'].astype(str).str.strip().str.replace('nan', '')\n",
    "sale['CITY'] = sale['CITY'].astype(str)\n",
    "sale = replace_city(sale, 'CITY', 'CITY')\n",
    "\n",
    "print(sale['GENDER'])\n",
    "print(sale['CITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f6e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale = sale[['MOBILE_PHONE', 'CUSTOMER_NAME', 'GENDER', 'CITY']]\n",
    "sale = sale.drop_duplicates(subset=['MOBILE_PHONE'])\n",
    "sale = modify_phone_num_column(sale, 'MOBILE_PHONE')\n",
    "\n",
    "dataset = pd.read_csv(output_folder + '/dataset_all_scored.csv')\n",
    "dataset['UID'] = dataset['UID'].astype(str)\n",
    "dataset = pd.merge(dataset, right=sale, how='left', left_on= 'UID',right_on='MOBILE_PHONE').drop(columns=['MOBILE_PHONE'], axis=1)\n",
    "del sale\n",
    "dataset.to_excel(output_folder + '/dataset_all_scored_info.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5bb17",
   "metadata": {},
   "source": [
    "# Customer Segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffdf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_direct = output_folder + \"/dataset_all_full_columns.csv\"\n",
    "data_scored_direct = output_folder + \"/dataset_all_scored.csv\"\n",
    "\n",
    "df_scored = pd.read_csv(data_scored_direct)[['UID', 'Score']]\n",
    "df = pd.read_csv(data_full_direct)#[['UID', setts.month_to_str, 'visit_freq_y', 'TTL']]\n",
    "df = pd.merge(df, right=df_scored, how='left', on='UID')\n",
    "\n",
    "df = df.rename(columns = { 'visit_freq_y' : 'Frequency', 'TTL' : 'Revenue'})\n",
    "# df['Recency']=-df[setts.month_to_str] #last month vua tinh\n",
    "\n",
    "# df['F']=pd.qcut(df['Frequency'], 5, labels = range(1,6), duplicates='drop').astype(int) # frequency= visit_freq_y\n",
    "# df['M']=pd.qcut(df['Revenue'], 5, labels = range(1,6), duplicates='drop').astype(int)  #monetary = TTL\n",
    "# df['Recency']=pd.qcut(df['Recency'], 5, labels = range(1,6), duplicates='drop').astype(int)\n",
    "\n",
    "# df['Frequency+Monetary']=(df['F']+df['M'])/2\n",
    "# df['Frequency+Monetary']=df['Frequency+Monetary'].round(0)\n",
    "\n",
    "# df['RFM']= df['Recency'].astype(str)+ df['Frequency+Monetary'].apply(lambda x: str(x)[0])\n",
    "\n",
    "customers = {'55' : 'Champions',\n",
    "'54' : 'Champions',\n",
    "'45' : 'Loyal Customers',\n",
    "'44' : 'Loyal Customers',\n",
    "'35' : 'Loyal Customers',\n",
    "'34' : 'Loyal Customers',\n",
    "'53' : 'Potential Loyalist',\n",
    "'52' : 'Potential Loyalist',\n",
    "'43' : 'Potential Loyalist',\n",
    "'42' : 'Potential Loyalist',\n",
    "'51' : 'New Customers',\n",
    "'41' : 'Promising',\n",
    "'31' : 'Need Attention',\n",
    "'32' : 'Need Attention',\n",
    "'33' : 'About to Sleep',\n",
    "'25' : \"Can't lose them\",\n",
    "'15' : \"Can't lose them\",\n",
    "'24' : 'At Risk',\n",
    "'23' : 'At Risk',\n",
    "'14' : 'At Risk',\n",
    "'13' : 'At Risk',\n",
    "'22' : 'Hibernating',\n",
    "'21' : 'Hibernating',\n",
    "'12' : 'Hibernating',\n",
    "'11' : 'Hibernating'}\n",
    "\n",
    "def classify_customer(customer_number):\n",
    "    if customer_number in customers:\n",
    "        return customers[customer_number]\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def seg_conditions(month_to, PM2ov, Score, total_buy):\n",
    "    if month_to < 7 and PM2ov == 1 and Score < 0.6 and total_buy == 1:\n",
    "        #return 'Temporary loyal customer'\n",
    "        return 'C2 Significant customer'\n",
    "    elif month_to < 7 and PM2ov == 1 and Score >= 0.6 and total_buy == 1:\n",
    "        #return 'Potential loyal customer'\n",
    "        return 'C1 Valued customer'\n",
    "    elif month_to < 7 and PM2ov == 1 and Score < 0.6 and total_buy > 1:\n",
    "        #return 'Passive Loyal customer'\n",
    "        return 'D2 Promise customer'\n",
    "    elif month_to < 7 and PM2ov == 1 and Score >= 0.6 and total_buy > 1:\n",
    "        #return 'Honda Fan / Loyal customer'\n",
    "        return 'D1 Honda Fan'\n",
    "    elif month_to < 7 and PM2ov == 0 and Score < 0.6:\n",
    "        #return 'Normal customer'\n",
    "        return 'B2 Sensitive customer'\n",
    "    elif month_to < 7 and PM2ov == 0 and Score >= 0.6:\n",
    "        #return 'Potential customer'\n",
    "        return 'B1 Potential customer'\n",
    "    elif month_to >= 7 and Score < 0.6:\n",
    "        #return 'Lost customer'\n",
    "        return 'A2 Lost customer'\n",
    "    elif month_to >= 7 and Score >= 0.6:\n",
    "        return 'A1 Attention customer'\n",
    "        #return 'Almost lost customer'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "def segment_customer(df, cot):\n",
    "    output = []\n",
    "    for index in range(len(df)):\n",
    "        row = df.iloc[index]\n",
    "        month_to = row[cot]\n",
    "        PM2ov = row['PM2ov']\n",
    "        Score = row['Score']\n",
    "        total_buy = row['total_buy']\n",
    "        #PM_vnum = row['PM_vnum']\n",
    "        output.append(seg_conditions(month_to, PM2ov, Score, total_buy))\n",
    "    return output\n",
    "    \n",
    "df['Customer segmentation'] = segment_customer(df, setts.month_to_str)\n",
    "#df['Customer segmentation'] = .apply(segment_customer(df, setts.month_to_str), axis=1)\n",
    "#df['Customer type']=df['RFM'].apply(lambda x:classify_customer(x))\n",
    "\n",
    "df.to_csv(output_folder + \"/dataset_all_RFM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488361c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfm_path = output_folder + \"/dataset_all_RFM.csv\"\n",
    "info_path = output_folder + \"/dataset_all_scored_info.xlsx\"\n",
    "\n",
    "#df_rfm = pd.read_csv(rfm_path)[['UID', 'Customer type','Customer segmentation']]\n",
    "df_rfm = pd.read_csv(rfm_path)[['UID','Customer segmentation']]\n",
    "df_info = pd.read_excel(info_path)\n",
    "\n",
    "df_info = pd.merge(df_info, df_rfm, how='left', on='UID')\n",
    "df_info.to_excel(output_folder + '/dataset_all_scored_info_RFM_result.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd011180",
   "metadata": {},
   "source": [
    "# Extract customer action data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "002c6e2b",
   "metadata": {},
   "source": [
    "# SERVICE\n",
    "\n",
    "def find_id_service(id, df_file, df_final):\n",
    "    df_id = df_file[df_file['UID'] == id]\n",
    "    non_df_id = df_file[df_file['UID'] != id]\n",
    "    \n",
    "    if len(df_id) > 0:\n",
    "        #df_id = df_id[['UID','DOC_DATE','JOB_TYPE_DESCRIPTION', 'DEALER_CODE']]\n",
    "        df_id = df_id.dropna(subset=['Event_date'])\n",
    "        df_id['Event_date'] = df_id['Event_date'].astype(str).apply(lambda x: x[4:6]+'/'+x[6:8]+'/'+x[0:4])\n",
    "        df_id['Event_date'] = df_id['Event_date'].astype(str).apply(\n",
    "            lambda x: datetime.strptime(x, '%m/%d/%Y').strftime('%m/%d/%Y'))\n",
    "        df_id = df_id.dropna(subset=['Action'])\n",
    "        df_id = df_id.drop_duplicates()\n",
    "    df_final = pd.concat([df_final, df_id], ignore_index=True)\n",
    "    return df_final, non_df_id\n",
    "\n",
    "service = pd.read_csv(root_folder + '/data/service_data.csv')[['VIN','DOC_DATE','JOB_TYPE_DESCRIPTION', 'DEALER_CODE']]\n",
    "sale = pd.read_csv(tmp1_folder + '/sale_uni_min.csv')[['VIN', 'UID']]\n",
    "\n",
    "cb = pd.merge(sale, right=service, how='left', left_on= 'VIN',right_on='VIN')\n",
    "cb = cb[['UID','DOC_DATE','JOB_TYPE_DESCRIPTION', 'DEALER_CODE']]\n",
    "del sale, service\n",
    "#find_id_service(num, id, self.service, df_final)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c15dab4",
   "metadata": {},
   "source": [
    "# SERVICE\n",
    "\n",
    "df_final = pd.DataFrame({'UID': [], 'Event_date': [], 'Action': [], 'DLR': []})\n",
    "id_list = cb['UID'].drop_duplicates().tolist()\n",
    "\n",
    "cb = cb.rename(columns={'DOC_DATE':'Event_date','JOB_TYPE_DESCRIPTION':'Action', 'DEALER_CODE':'DLR'})\n",
    "print(cb.columns)\n",
    "\n",
    "backup_break = 10000\n",
    "i = 0\n",
    "backup_count = 1\n",
    "for id in id_list:\n",
    "    if i == backup_break:\n",
    "        df_final.to_csv(output_folder + \"/customer_action_service_backup_%s.csv\" % (str(backup_count)), index=False)\n",
    "        df_final = pd.DataFrame({'UID': [], 'Event_date': [], 'Action': [], 'DLR': []})\n",
    "        i = 0\n",
    "        backup_count += 1\n",
    "    else:\n",
    "        #print(id, len(df_final), len(cb))\n",
    "        df_final, cb = find_id_service(id, cb, df_final)\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "print(df_final.head())\n",
    "df_final.to_csv(output_folder + \"/customer_action_service_final.csv\", index=False)\n",
    "#del cb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0646e6af",
   "metadata": {},
   "source": [
    "cb = cb.rename(columns={'DOC_DATE':'Event_date','JOB_TYPE_DESCRIPTION':'Action', 'DEALER_CODE':'DLR'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae3ea9fa",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "def find_id_sale(id, df_file, df_final):\n",
    "    df_id = df_file[df_file['UID'] == id]\n",
    "    df_id_rev = df_file[df_file['UID'] != id]\n",
    "    if len(df_id) > 0:\n",
    "        df_id = df_id.rename(columns={'WARRANTY_START_DATE':'Event_date','MODEL_TEXT_1':'Action', 'DEALER_CODE':'DLR'})\n",
    "        df_id['Event_date'] = df_id['Event_date'].astype(str).str.replace('-', '').apply(lambda x: x[4:6]+'/'+x[6:8]+'/'+x[0:4])\n",
    "        df_id['Event_date'] = df_id['Event_date'].astype(str).apply(\n",
    "            lambda x: datetime.strptime(x, '%m/%d/%Y').strftime('%m/%d/%Y'))\n",
    "        df_id['Action'] = df_id['Action'].apply(lambda x: 'Buy '+x)\n",
    "    df_final = pd.concat([df_final, df_id], ignore_index=True)\n",
    "    return df_final, df_id_rev\n",
    "\n",
    "df_final = pd.DataFrame({'UID': [], 'Event_date': [], 'Action': [], 'DLR': []})\n",
    "id_list = cb['UID'].drop_duplicates().tolist()\n",
    "cb = pd.read_csv(tmp1_folder + '/sale_uni_min.csv')\n",
    "cb = cb[['UID','WARRANTY_START_DATE','MODEL_TEXT_1', 'DEALER_CODE']]\n",
    "cb = cb.sort_values(by=['UID'])\n",
    "backup_break = 10000\n",
    "backup_count = 1\n",
    "i = 0\n",
    "for id in sorted(id_list):\n",
    "    if i == backup_break:\n",
    "        df_final.to_csv(output_folder + \"/customer_action_sale_backup_%s.csv\" % (str(backup_count)), index=False)\n",
    "        df_final = pd.DataFrame({'UID': [], 'Event_date': [], 'Action': [], 'DLR': []})\n",
    "        i = 0\n",
    "        backup_count += 1\n",
    "    else:\n",
    "        print(id, len(df_final), len(cb))\n",
    "        df_final, cb = find_id_sale(id, cb, df_final)\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "df_final.to_csv(output_folder + \"/customer_action_sale_final.csv\", index=False)\n",
    "print(df_final.head())\n",
    "del cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60af0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df_final.drop_duplicates()\n",
    "# df_final.to_csv(output_folder + \"/customer_action_service_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d636f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final_backup = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4366c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
